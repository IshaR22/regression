{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e622e490-2323-4b8e-96fa-b53b52eea457",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide\n",
    "an example of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52997fe-b7ca-4a86-a8cf-7b2b525a48b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE LINEAR REGRESSION\n",
    "1. Simple linear regression has one independent variable.\n",
    "2. Simple linear regression equation: h0= Q0 + Q1x1\n",
    "3. Simple linear regression is used to model the linear relationship between a single independent variable and a dependent variable.\n",
    "example price of the house on the bases of the size of the house\n",
    "the house price h0 = Q0 + Q1x1\n",
    "\n",
    "MULTIPLE LINEAR REGRESSION\n",
    "1. Multiple linear regression has two or more independent variables.\n",
    "2. Multiple linear regression equation: y = a + b1x1 + b2x2 + ... + bnxn\n",
    "3. Multiple linear regression is used to model the linear relationship between multiple independent variables and a dependent variable.\n",
    "example price of the house on the bases of the size , location , no. of rooms.\n",
    "House Price = a + b1 × House Size + b2 × Number of Bedrooms + b3 × House loacation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084ca4e-c9d6-4856-ab29-9bae3ae2c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a70995-fb73-4743-988e-7327ead0706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The key assumptions of linear regression are:\n",
    "\n",
    "1. Linearity: The relationship between the independent variable(s) and the dependent variable should \n",
    "be linear.\n",
    "can be checked by ploting a scatter plot \n",
    "    \n",
    "2. Normality: The residuals (the differences between the observed values and the predicted values)\n",
    "should be normally distributed.\n",
    "by creating a histogram or a Q-Q plot of the residuals.\n",
    "\n",
    "3. Homoscedasticity: The variance of the residuals should be constant across all levels of the \n",
    "independent variable(s)\n",
    "check by creating a scatter plot of the residuals against the predicted values.\n",
    "\n",
    "4. Independence: The residuals should be independent of each other.\n",
    "check this by creating a scatter plot of the residuals against the order of the data.\n",
    "\n",
    "5. Multicollinearity (for multiple linear regression): The independent variables should not be \n",
    "highly correlated with each other.\n",
    "You can check this by calculating the variance inflation factor (VIF) for each independent variable.\n",
    "A VIF greater than 5 or 10 indicates a potential multicollinearity problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04852e-f18d-48bf-b145-62fd21309f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example \n",
    "usinga real-world scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1835198-5452-415e-8813-e04ec9b7edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope :\n",
    "\n",
    "The slope represents the change in the dependent variable (y) for a one-unit change in the \n",
    "independent variable (x)\n",
    "A positive slope means that as the independent variable increases, the dependent variable tends to\n",
    "increase.\n",
    "A negative slope means that as the independent variable increases, the dependent variable tends to \n",
    "decrease.\n",
    "The magnitude of the slope represents the rate of change between the variables.\n",
    "\n",
    "Intercept:\n",
    "\n",
    "The intercept represents the value of the dependent variable (y) when the independent variable (x) \n",
    "is zero.\n",
    "The intercept is the point where the regression line intersects the y-axis.\n",
    "The intercept can be interpreted as the baseline or starting value of the dependent variable \n",
    "when the independent variable is not present.\n",
    "\n",
    "Suppose you want to predict the sales revenue (dependent variable) of a company based on its \n",
    "advertising expenditure (independent variable). You fit a simple linear regression model and obtain \n",
    "the following equation:\n",
    "\n",
    "Sales Revenue = 50,000 + 2,000 × Advertising Expenditure\n",
    "\n",
    "Interpretation:\n",
    "Slope (b = 2,000): For every \n",
    "2,000, holding all other factors constant.\n",
    "Intercept (a = 50,000): When the advertising expenditure is \n",
    "50,000. This represents the baseline sales revenue without any advertising.\n",
    "In this example, the slope of 2,000 indicates a strong positive relationship between advertising \n",
    "expenditure and sales revenue. The intercept of 50,000 suggests that the company has a baseline sales\n",
    "revenue of $50,000 even without any advertising.\n",
    "\n",
    "This interpretation can help the company understand the impact of advertising on their sales and\n",
    "make informed decisions about their marketing strategies and budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7774b29-4635-49fb-9386-bc4b43e2d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Explain the concept of gradient descent.How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce6814b-73af-46a6-b3eb-ffe88536d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Descent:\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm that starts with an initial set of parameters and iteratively updates them to minimize the loss function.\n",
    "The algorithm calculates the gradient of the loss function with respect to each parameter, which \n",
    "represents the direction of the steepest increase in the loss function.\n",
    "The parameters are then updated in the opposite direction of the gradient, proportional to the \n",
    "learning rate, to move towards the minimum of the loss function.\n",
    "Use in Machine Learning:\n",
    "\n",
    "In machine learning, gradient descent is commonly used to train models, such as linear regression, \n",
    "logistic regression, neural networks, and many others.\n",
    "The loss function in machine learning represents the difference between the predicted output and the\n",
    "true output, and the goal is to minimize this loss function.\n",
    "By applying gradient descent, the model parameters are updated iteratively to reduce the loss \n",
    "function, ultimately converging to the optimal set of parameters that best fit the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31dccf1-0138-4e51-8f1e-b16b85c49924",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7562b4-85dc-406b-a97a-962b4f152ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "The multiple linear regression model is an extension of the simple linear regression model, where the dependent variable is predicted based on two or more independent variables.\n",
    "\n",
    "The multiple linear regression model can be expressed as:\n",
    "y = a + b1x1 + b2x2 + ... + bnxn\n",
    "Y is the dependent variable\n",
    "X1, X2, ..., Xk are the independent variables\n",
    "B0 is the intercept\n",
    "B1, B2, ..., Bk are the regression coefficients (slopes) for each independent variable\n",
    "\n",
    "SIMPLE LINEAR REGRESSION\n",
    "1. Simple linear regression has one independent variable.\n",
    "2. Simple linear regression equation: h0= Q0 + Q1x1\n",
    "3. Simple linear regression is used to model the linear relationship between a single independent variable and a dependent variable.\n",
    "example price of the house on the bases of the size of the house\n",
    "the house price h0 = Q0 + Q1x1\n",
    "\n",
    "MULTIPLE LINEAR REGRESSION\n",
    "1. Multiple linear regression has two or more independent variables.\n",
    "2. Multiple linear regression equation: y = a + b1x1 + b2x2 + ... + bnxn\n",
    "3. Multiple linear regression is used to model the linear relationship between multiple independent \n",
    "variables and a dependent variable.\n",
    "example price of the house on the bases of the size , location , no. of rooms.\n",
    "House Price = a + b1 × House Size + b2 × Number of Bedrooms + b3 × House loacation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d41ee91-19cb-46e8-be14-49dac8c53110",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of multicollinearityin multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a1fa1-6ec0-4b91-b583-aea30f26818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity is a condition that arises in multiple linear regression when two or\n",
    "more independent variables in the model are highly correlated with each other. \n",
    "There are several ways to detect multicollinearity in a multiple linear regression model:\n",
    "1. Correlation Matrix:\n",
    "Examine the correlation coefficients between the independent variables.\n",
    "If the correlation coefficients are high (typically above 0.7 or 0.8), it indicates the presence of multicollinearity.\n",
    "\n",
    "2. Variance Inflation Factor (VIF):\n",
    "The VIF measures the degree to which the variance of a coefficient is inflated due to \n",
    "multicollinearity.\n",
    "A VIF value greater than 5 or 10 (depending on the threshold used) suggests the presence of \n",
    "multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "If multicollinearity is detected in a multiple linear regression model, there are several approaches\n",
    "to address the issue:\n",
    "\n",
    "1.Removing Highly Correlated Variables:\n",
    "Identify the independent variables that are highly correlated and remove one or more of them from \n",
    "the model.\n",
    "2. Centering and Scaling the Variables:\n",
    "Center the independent variables by subtracting their mean from each observation.\n",
    "3.Principal Component Regression (PCR):\n",
    "\n",
    "PCR involves transforming the original independent variables into a set of uncorrelated principal\n",
    "components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971edee7-8349-4ac9-af4d-040ca70937f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a612fc-a901-41d3-bbd6-6411e39d2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "The polynomial regression model is an extension of the linear regression model, where the \n",
    "relationship between the independent variable(s) and the dependent variable is modeled using a \n",
    "polynomial function. In a polynomial regression model, the independent variable(s) are raised to \n",
    "higher powers, allowing the model to capture non-linear relationships.\n",
    "\n",
    "The general form of a polynomial regression model with a single independent variable (X) can be \n",
    "expressed as:\n",
    "\n",
    "Y = B0 + B1X + B2X^2 + ... + BnX^n + ERROR\n",
    "WHERE:\n",
    "Y is the dependent variable\n",
    "X is the independent variable\n",
    "B0, B1, B2, ..., Bn are the regression coefficients\n",
    "\n",
    "1.Linear regression assumes a linear relationship between the independent variable(s) and the \n",
    "dependent variable.\n",
    "2.Linerar  regression is less flexible \n",
    "3.In linear regression, the model complexity is determined by the number of independent variables.\n",
    "\n",
    "\n",
    "1.Polynomial regression can capture non-linear relationships by including higher-order terms (e.g., \n",
    "X^2, X^3) in the model.\n",
    "2.Polynomial regression is more flexible than linear regression, as it can fit a wider range of \n",
    "functional forms, including quadratic, cubic, and higher-order polynomial relationships.\n",
    "3. In polynomial regression, the model complexity is determined by the highest power of the \n",
    "independent variable included in the model, which needs to be carefully selected to avoid \n",
    "overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5199ca-f37d-43ad-8274-6fd1fd560a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bced4fde-0c6e-4211-8e74-7b4885462f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of Polynomial Regression:\n",
    "\n",
    "1. Flexibility: Polynomial regression can model a wider range of non-linear relationships between \n",
    "the independent variable(s) and the dependent variable, making it more flexible than linear \n",
    "regression.\n",
    "2. Improved Fit: Polynomial regression can often provide a better fit to the data, especially when \n",
    "the underlying relationship is non-linear. This can lead to improved predictive accuracy.\n",
    "\n",
    "3. Capturing Complex Patterns: Polynomial regression can capture more complex patterns in the data,\n",
    "such as quadratic, cubic, or higher-order relationships, which linear regression may not be able to \n",
    "model effectively.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "1. Overfitting: Polynomial regression models can be prone to overfitting, especially when the degree\n",
    "of the polynomial is too high. This can lead to poor generalization to new, unseen data.\n",
    "2. Interpretability: The interpretation of the regression coefficients in polynomial regression is\n",
    "more complex compared to linear regression, as each coefficient represents the change in the \n",
    "dependent variable for a one-unit change in the corresponding power of the independent variable.\n",
    "3. Multicollinearity: When the independent variables are highly correlated, the higher-order terms \n",
    "in the polynomial regression model can lead to multicollinearity, which can negatively impact the \n",
    "stability and reliability of the model.\n",
    "\n",
    "Situations to Prefer Polynomial Regression:\n",
    "\n",
    "1. Non-linear Relationships: When the relationship between the independent variables and the \n",
    "dependent variable is known or suspected to be non-linear, polynomial regression can be a better \n",
    "choice than linear regression.\n",
    "\n",
    "2. Exploratory Data Analysis: Polynomial regression can be useful in the EDA stage, as it can help\n",
    "identify the presence and nature of non-linear relationships in the data.\n",
    "\n",
    "Improved Predictive Accuracy: If the goal is to maximize the predictive accuracy of the model, \n",
    "and the data exhibits non-linear patterns, polynomial regression may outperform linear regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d99b489-cde1-4c7e-8243-ab9489071550",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c925f9d7-9e9a-49ea-b3b3-df38222c5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.           \n",
    "RMSA=10      \n",
    "The RMSE of 10 for Model A implies that there are larger errors present in the predictions, as RMSE \n",
    "penalizes larger errors more heavily by squaring them.\n",
    "B\n",
    "MAE=8\n",
    "The MAE of 8 for Model B implies that on average, there is an absolute error of 8 between the \n",
    "predicted and actual values.\n",
    "\n",
    "If we are more concerned with average performance and not as worried about particularly large errors,\n",
    "Model B with an MAE of 8 may be preferred.\n",
    "If larger errors are a significant concern and detrimental, the RMSE gives more weight to those \n",
    "issues. In this case, evaluating Model A within the context of those larger errors would be crucial, \n",
    "however, Model A with an RMSE of 10 might signal larger errors that could be problematic in specific\n",
    "applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780b7d61-4751-4818-96b9-5f3f464d11b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3a1bd-90c8-4e3a-bc68-cd30fc7255c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing the performance of two regularized linear models \n",
    "### Ridge Regularization (Model A)\n",
    "- Ridge regularization, also known as L2 regularization, adds a penalty equal to the sum of the \n",
    "squared coefficients magnitudes multiplied by a regularization parameter. \n",
    "- The effect is to shrink the coefficients of the model, but not necessarily zero them out, making \n",
    "Ridge useful for scenarios where you believe many features contribute to the output in some way.\n",
    "- Regularization parameter,(0.1), indicates a relatively mild regularization effect.\n",
    "\n",
    "### Lasso Regularization (Model B)\n",
    "- Lasso regularization, also known as L1 regularization, adds a penalty equal to the sum of the \n",
    "absolute values of the coefficients multiplied by a regularization parameter \\(\\alpha\\).\n",
    "- The effect is to drive some coefficients to zero, effectively performing feature selection. This \n",
    "makes Lasso useful when you suspect that only a few features are important.\n",
    "- Regularization parameter(0.5), indicates a stronger regularization effect compared to\n",
    "Ridge.\n",
    "\n",
    "To decide which model is better, consider these aspects:\n",
    "\n",
    "1.Model Performance:\n",
    "Evaluate both models using standard performance metrics like MSE,MAE,RMSE, R² score, etc., using \n",
    "cross-validation to ensure consistency.\n",
    "\n",
    "2.Sparsity of the Model:\n",
    "If model interpretability and feature selection are important, Lasso (Model B) might be more \n",
    "appropriate due to its tendency to zero out less important features.\n",
    "\n",
    "Let's assume both models yield similar performance metrics (MSE, MAE, etc.) on a validation set. \n",
    "The choice then hinges on additional considerations:\n",
    "   \n",
    "1.Handling Multicollinearity:\n",
    "Model A (Ridge):Ridge regularization is generally better at handling multicollinearity since it \n",
    "distributes the penalty across all weights, ensuring none are driven to zero. It's more appropriate \n",
    "if you believe all features contribute some information.\n",
    "\n",
    "2.Stability and Predictive Power:\n",
    "Stability under different splits of the training data can be an indicator. Lasso might lead to models\n",
    "that are less stable due to the zeroing out of coefficients, whereas Ridge tends to produce more \n",
    "stable solutions.\n",
    "\n",
    "### Trade-offs and Limitations:\n",
    "- Lasso (Model B):\n",
    "Stronger regularization parameter(0.5) might over-regularize, particularly if the true model has \n",
    "more than a few important non-zero coefficients.\n",
    "Can be less stable with different data splits, potentially leading to variable feature selection \n",
    "results.\n",
    "Might underperform if the true relationship involves many non-zero coefficients.\n",
    "\n",
    "Ridge (Model A):\n",
    "Mild regularization parameter (0.1) could lead to less bias reduction but better retains features. \n",
    "Does not perform feature selection, which might lead to less interpretable models but potentially\n",
    "more robust to small changes in data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
